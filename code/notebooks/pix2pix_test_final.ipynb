{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from math import log10, sqrt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Do:\n",
    "1. Make the configuration tweaks mentioned below\n",
    "2. Run the model\n",
    "3. SAVE THE LOSS FUNCTIONS!! **DONT FORGET**\n",
    "4. Run predict() and SAVE THE FIRST 5 images (eval values are included in the image)!! **DONT FORGET**\n",
    "5. Upload loss + images to the drive\n",
    "\n",
    "\n",
    "### Configurations:\n",
    "\n",
    "- Try to shoot for epoch 10 (less if takes too much time)\n",
    "- Batch size 10 or do whatever the optimal one was that Elaine found. I think it was 20.\n",
    "- Keep everything as default and only change one parameter at a time<br>\n",
    "- **6 Configurations total**\n",
    "\n",
    "Loss Functions:<br>\n",
    "** ignore BCE because its not learning anything\n",
    "1. L1 Loss only\n",
    "2. L2 Loss only (MSE)\n",
    "\n",
    "NGF: (this is the number of filters in the last conv layer)<Br>\n",
    "**run this with L1 only**\n",
    "3. 16 (should do poorly)\n",
    "4. 64 (default)\n",
    "5. 128\n",
    "6. 256 (if you can't run it, scrap it)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import datetime\n",
    "\n",
    "\n",
    "# Hyperparameters (Put here for now. Probably should find a way to organize them)\n",
    "SIZE = 256\n",
    "INPUT_CHANNELS = 3\n",
    "OUTPUT_CHANNELS = 3\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 5\n",
    "LEARNING_RATE = 1e-4\n",
    "BETA1 = 0.5 # Beta1 hyperparam for Adam optimizers\n",
    "SHUFFLE = True\n",
    "\n",
    "\n",
    "class NeuralModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load in default parameters\n",
    "        self.size = SIZE\n",
    "        self.input_c = INPUT_CHANNELS\n",
    "        self.output_c = OUTPUT_CHANNELS\n",
    "        self.num_epochs = NUM_EPOCHS\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.lr = LEARNING_RATE\n",
    "        self.beta1 = BETA1\n",
    "        self.shuffle = SHUFFLE\n",
    "\n",
    "        current_time = datetime.datetime.now() - datetime.timedelta(hours=8)\n",
    "        print(\"Starting at:\", current_time.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "\n",
    "    def forward(self, obs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def train(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def prepare_batch(self, data):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List\n",
    "import functools\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils import data\n",
    "import h5py\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "NUM_DOWNS = 8\n",
    "NGF = 64\n",
    "LAMBDA_L1 = 100\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, x, y=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.y is None:\n",
    "            return self.x[index]\n",
    "        else:\n",
    "            return self.x[index], self.y[index]\n",
    "\n",
    "\n",
    "# Reference: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/\n",
    "class Pix2PixUnet(NeuralModel, nn.Module):\n",
    "    \"\"\"Create a Unet-based generator\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Construct a Unet generator\n",
    "        Parameters:\n",
    "            input_nc (int)  -- the number of channels in input images\n",
    "            output_nc (int) -- the number of channels in output images\n",
    "            num_downs (int) -- the number of downsamplings in UNet. For example, # if |num_downs| == 7,\n",
    "                                image of size 128x128 will become of size 1x1 # at the bottleneck\n",
    "            ngf (int)       -- the number of filters in the last conv layer\n",
    "            norm_layer      -- normalization layer\n",
    "        We construct the U-Net from the innermost layer to the outermost layer.\n",
    "        It is a recursive process.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # hyper params\n",
    "        self.num_downs = NUM_DOWNS\n",
    "        self.ngf = NGF\n",
    "        self.lambda_L1 = LAMBDA_L1\n",
    "        self.norm_layer = nn.BatchNorm2d\n",
    "\n",
    "        # construct unet structure\n",
    "        unet_block = UnetSkipConnectionBlock(self.ngf * 8, self.ngf * 8, input_nc=None, submodule=None, norm_layer=self.norm_layer, innermost=True)  # add the innermost layer\n",
    "        for i in range(self.num_downs - 5):          # add intermediate layers with ngf * 8 filters\n",
    "            unet_block = UnetSkipConnectionBlock(self.ngf * 8, self.ngf * 8, input_nc=None, submodule=unet_block, norm_layer=self.norm_layer, use_dropout=False)\n",
    "        # gradually reduce the number of filters from ngf * 8 to ngf\n",
    "        unet_block = UnetSkipConnectionBlock(self.ngf * 4, self.ngf * 8, input_nc=None, submodule=unet_block, norm_layer=self.norm_layer)\n",
    "        unet_block = UnetSkipConnectionBlock(self.ngf * 2, self.ngf * 4, input_nc=None, submodule=unet_block, norm_layer=self.norm_layer)\n",
    "        unet_block = UnetSkipConnectionBlock(self.ngf, self.ngf * 2, input_nc=None, submodule=unet_block, norm_layer=self.norm_layer)\n",
    "        self.model = UnetSkipConnectionBlock(self.output_c, self.ngf, input_nc=self.input_c, submodule=unet_block, outermost=True, norm_layer=self.norm_layer)  # add the outermost layer\n",
    "        \n",
    "        self.device = torch.device(\n",
    "            f\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        print(f\"Device: {self.device}\")\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, betas=(self.beta1, 0.999))\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.criterionL1 = torch.nn.L1Loss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        # load data from file\n",
    "        with h5py.File(\"../data_950_0to1.h5\", 'r') as hf:\n",
    "            data_x = hf['x'][:]\n",
    "            data_y = hf['y'][:]\n",
    "\n",
    "        # 80/20 split\n",
    "        # data_tr, data_te = data_obs.split()\n",
    "        train_ind = int(len(data_x) * 0.8)\n",
    "        data_tr = Dataset(data_x[:train_ind], data_y[:train_ind])\n",
    "\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            data_tr,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=self.shuffle\n",
    "        )\n",
    "        \n",
    "        train_losses = []\n",
    "        loss_gen_list = []\n",
    "        loss_L1_list = []\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            start_time1 = time.time()\n",
    "\n",
    "            epoch_loss = torch.tensor(0.0, device=self.device)\n",
    "\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                start_time2 = time.time()\n",
    "\n",
    "                data, target = batch\n",
    "\n",
    "                outputs = self.model(data.float().to(self.device))\n",
    "                \n",
    "                # if you want to see the progress of the outputs\n",
    "#                 if batch_idx % 20 == 0:\n",
    "#                     out = outputs.detach().cpu().numpy()\n",
    "#                     tar = target.detach().cpu().numpy()\n",
    "#                     plt.subplot(1, 2, 1)\n",
    "#                     plt.imshow(out.transpose((0, 2, 3, 1))[0])\n",
    "#                     plt.subplot(1, 2, 2)\n",
    "#                     plt.imshow(tar.transpose((0, 2, 3, 1))[0])\n",
    "#                     plt.show()\n",
    "\n",
    "                loss_gen = self.criterion(outputs, target.float().to(self.device))\n",
    "                loss_L1 = self.criterionL1(outputs, target.float().to(self.device)) * self.lambda_L1\n",
    "                loss = loss_gen + loss_L1\n",
    "                \n",
    "                loss_gen_list.append(loss_gen)\n",
    "                loss_L1_list.append(loss_L1)\n",
    "                \n",
    "                # Aggregate loss across mini-batches (per epoch)\n",
    "                epoch_loss += loss\n",
    "\n",
    "                # Backprop and perform Adam optimisation\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                if batch_idx % 38 == 0:\n",
    "                    print(\n",
    "                    f\"Epoch: {epoch}\\tBatch: {batch_idx}\\tGen Loss: {loss_gen:.5f}\\tL1 Loss: {loss_L1:.5f}\"\n",
    "                    f\"\\tTotal Time: {(time.time() - start_time2)/60:.3f} minutes\"\n",
    "                    )\n",
    "\n",
    "            print(\n",
    "                f\"Epoch: {epoch}\\tTrain Loss: {epoch_loss/len(train_loader):.5f}\"\n",
    "                f\"\\tTotal Time: {(time.time() - start_time1)/60:.3f} minutes\"\n",
    "            )\n",
    "\n",
    "            train_losses.append(epoch_loss / len(train_loader))\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(2, figsize=(15,15))\n",
    "        fig.suptitle(\"Loss Plots\")\n",
    "        ax1.plot(range(len(loss_gen_list)), loss_gen_list, alpha = 0.7)\n",
    "        ax1.set_title(\"Gen Loss\")\n",
    "        ax2.plot(range(len(loss_gen_list)), loss_L1_list, alpha = 0.7)\n",
    "        ax2.set_title(\"L1 Loss\")\n",
    "        plt.show()\n",
    "        \n",
    "        return train_losses\n",
    "    \n",
    "    \n",
    "    def predict(self):\n",
    "        # load data from file\n",
    "        with h5py.File(\"../data_950_0to1.h5\", 'r') as hf:\n",
    "            data_x = hf['x'][:]\n",
    "            data_y = hf['y'][:]\n",
    "        \n",
    "        color_distance = []\n",
    "        \n",
    "        # 80/20 split\n",
    "        # data_tr, data_te = data_obs.split()\n",
    "        train_ind = int(len(data_x) * 0.8)\n",
    "        data_te = Dataset(data_x[train_ind:], data_y[train_ind:])\n",
    "            \n",
    "        test_loader = DataLoader(\n",
    "            data_te,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            start_time2 = time.time()\n",
    "\n",
    "            data, target = batch\n",
    "            x = data.detach().cpu().numpy().transpose((0, 2, 3, 1))[0]\n",
    "            y = target.detach().cpu().numpy().transpose((0, 2, 3, 1))[0]\n",
    "            \n",
    "            outputs = self.model(data.float().to(self.device))\n",
    "            \n",
    "            y_hat = outputs.detach().cpu().numpy().transpose((0, 2, 3, 1))[0]\n",
    "            \n",
    "            dist = 0\n",
    "            for i in range(0, len(y)):\n",
    "                for j in range(0,len(y[i])):\n",
    "                    dist += math.sqrt((y[i][j][0] - y_hat[i][j][0])**2 + (y[i][j][1] - y_hat[i][j][1])**2 +\n",
    "                                      (y[i][j][2] - y_hat[i][j][2])**2)\n",
    "            dist = dist/len(y)\n",
    "            color_distance.append(dist)\n",
    "\n",
    "            if batch_idx < 5:  #SELECT INTERESTING IMAGES IF YOU WANT HERE\n",
    "                fig, ax = plt.subplots(1, 3, figsize=(16, 5))\n",
    "                ax[0].axis('off')\n",
    "                ax[0].set_title('$X$')\n",
    "                ax[0].imshow(x)\n",
    "                ax[1].axis('off')\n",
    "                ax[1].set_title('$\\hat{Y}$')\n",
    "                ax[1].imshow(y_hat)\n",
    "                ax[2].axis('off')\n",
    "                ax[2].set_title('$Y$')\n",
    "                ax[2].imshow(y)\n",
    "                \n",
    "                \n",
    "                # calculate Evaluation and attach it to the image\n",
    "                color_dist = \"Color difference: %d\" % (dist)\n",
    "                MSE = \"MSE: %f\" % (np.mean((y_hat - y) ** 2) )\n",
    "                PSNR = \"PSNR: %f\" % (self.psnr(y, y_hat))\n",
    "                SSIM = \"SSIM: %f\" % (self.ssim(y, y_hat))\n",
    "                fig.suptitle(color_dist + '      ' + MSE + '      ' +  PSNR + '      ' +  SSIM, y=0.09)\n",
    "                plt.show()\n",
    "#                 print(MSE)\n",
    "#                 print(PSNR)\n",
    "#                 print(SSIM)\n",
    "#                 print(\"Color difference: %d\" % (dist))\n",
    "\n",
    "    \n",
    "        print(\"Average color difference: %d\" % (np.mean(np.array(color_distance))))\n",
    "        print(\"Max color difference: %d\" % (max(color_distance)))\n",
    "        print(\"Min color difference: %d\" % (min(color_distance)))\n",
    "        \n",
    "        print(color_distance)\n",
    "    \n",
    "    @staticmethod\n",
    "    def psnr(original, compressed): \n",
    "        mse = np.mean((original - compressed) ** 2) \n",
    "        if(mse == 0):  # MSE is zero means no noise is present in the signal . \n",
    "                      # Therefore PSNR have no importance. \n",
    "            return 100\n",
    "\n",
    "        max_pixel = 255.0\n",
    "        psnr = 20 * log10(max_pixel / sqrt(mse)) \n",
    "        return psnr\n",
    "    \n",
    "    @staticmethod\n",
    "    def ssim(y, y_hat):\n",
    "        return ssim(y, y_hat, data_range=1, multichannel=True)\n",
    "\n",
    "\n",
    "class UnetSkipConnectionBlock(nn.Module):\n",
    "    \"\"\"Defines the Unet submodule with skip connection.\n",
    "        X -------------------identity----------------------\n",
    "        |-- downsampling -- |submodule| -- upsampling --|\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, outer_nc, inner_nc, input_nc=None,\n",
    "                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "        \"\"\"Construct a Unet submodule with skip connections.\n",
    "        Parameters:\n",
    "            outer_nc (int) -- the number of filters in the outer conv layer\n",
    "            inner_nc (int) -- the number of filters in the inner conv layer\n",
    "            input_nc (int) -- the number of channels in input images/features\n",
    "            submodule (UnetSkipConnectionBlock) -- previously defined submodules\n",
    "            outermost (bool)    -- if this module is the outermost module\n",
    "            innermost (bool)    -- if this module is the innermost module\n",
    "            norm_layer          -- normalization layer\n",
    "            use_dropout (bool)  -- if use dropout layers.\n",
    "        \"\"\"\n",
    "        super(UnetSkipConnectionBlock, self).__init__()\n",
    "        self.outermost = outermost\n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "        if input_nc is None:\n",
    "            input_nc = outer_nc\n",
    "        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n",
    "                             stride=2, padding=1, bias=use_bias)\n",
    "        downrelu = nn.LeakyReLU(0.2, True)\n",
    "        downnorm = norm_layer(inner_nc)\n",
    "        uprelu = nn.ReLU(True)\n",
    "        upnorm = norm_layer(outer_nc)\n",
    "\n",
    "        if outermost:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1)\n",
    "            down = [downconv]\n",
    "            up = [uprelu, upconv, nn.Tanh()]\n",
    "            model = down + [submodule] + up\n",
    "        elif innermost:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1, bias=use_bias)\n",
    "            down = [downrelu, downconv]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            model = down + up\n",
    "        else:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1, bias=use_bias)\n",
    "            down = [downrelu, downconv, downnorm]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "\n",
    "            if use_dropout:\n",
    "                model = down + [submodule] + up + [nn.Dropout(0.5)]\n",
    "            else:\n",
    "                model = down + [submodule] + up\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.outermost:\n",
    "            return self.model(x)\n",
    "        else:   # add skip connections\n",
    "            return torch.cat([x, self.model(x)], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Pix2PixUnet()\n",
    "\n",
    "tr_losses = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict()  # graphs first 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(data_x3.transpose((0, 2, 3, 1))[0])\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
